# Prerequisites
Install sqlalchemy and psycopg2 for database interaction

```bash
pip install pandas sqlalchemy psycopg2-binary
```

backend/ingest_engine.py includes:

- Context Manager: Safely handles database sessions
- Run Tracking: Creates a new entry in recon_runs before processing
- Normalization: Converts the "Source B" quirks (e.g., 'B/S') into the canonical format
- JSONB Packing: Packs the raw data into the normalized_data JSONB column as per your schema



# How to Test
- Generate Data: Run your tools/generate_test_data.py script first to create the CSVs
- Database: Ensure your Postgres container/instance is running and you have run the 01_init... and 02_seed... SQL scripts
- Run the ingestion engine:
```bash
python backend/ingest_engine.py
```
- Verify: Open your SQL client and run:
```sql
SELECT * FROM recon.recon_runs;
```
```sql
SELECT * FROM recon.recon_records LIMIT 5;
```



## Usage & Verification for Difference Engine
### Pre-requisites:
- Database is running
- ingest_engine.py has run successfully (check recon.recon_runs for a COMPLETED status).

### Run the Difference Engine:

```bash
python backend/diff_engine.py
```

### Verify Results:
- Check the recon.data_differences table with the following SQL query. This confirms your "Test Data Generator" noise (rounding errors, missing records) is being caught correctly.
```sql
SELECT 
    r.source_a_ref_id, 
    d.field_name, 
    d.value_a, 
    d.value_b, 
    d.diff_type 
FROM recon.data_differences d
JOIN recon.recon_records r ON d.record_id = r.record_id
LIMIT 20;
```

## Attribution Engine
### Pre-requisites:
```bash
pip install scikit-learn
```
backend/attribution_engine.py 
- Trains (or Mocks) a Model: It prepares a RandomForestClassifier. (For this initial run, it uses heuristic rules to simulate a trained model since we don't have historical labeled data yet).
- Feature Engineering: It converts raw difference data into "features" (e.g., Is the difference < 0.01? Is it a date field?).
- Confidence Scoring: It assigns a probability score to every prediction.
- Database Write: It populates the attributions table.

#### How to Validate this Step
- Run the Script:
```bash
python backend/attribution_engine.py
```

- Check the Database
Run this SQL query to see the "Data Quality Gate" in action. You should see ROUNDING_DIFF (functional) separate from MISSING_SOURCE (non-functional).

```sql
SELECT 
    r.code, 
    a.confidence_score, 
    a.status, 
    COUNT(*) 
FROM recon.attributions a
JOIN recon.reason_codes r ON a.reason_id = r.reason_id
GROUP BY r.code, a.confidence_score, a.status
ORDER BY a.status, a.confidence_score DESC;
```

## FastAPI (Python) BFF Interface that serves the "Review Queue" API endpoint for your frontend

This is the production-grade FastAPI (BFF) implementation. It is fully asynchronous, uses strictly typed Pydantic models, and implements the 4-Eye Check logic we defined in the requirements.

### File Structure
Create a bff/ directory in your project root.

### Prerequisites
You need the async Postgres driver (asyncpg) and fastapi.

```bash
pip install fastapi uvicorn asyncpg pydantic-settings
```

5. API Logic (bff/routers/workflow.py)
This router implements the Governance Workflow. It handles:

Fetching "Unknown" items.

Processing overrides.

Writing to the Audit Trail.

### How to Run & Test
#### Start the Server:
```bash
python bff/main.py
```

#### Open API Docs
Go to http://localhost:8000/docs. You will see the Swagger UI generated automatically

### Test the Queue
- Expand GET /workflow/queue
- Click Try it out $\rightarrow$ Execute.
- You should see the JSON list of "UNKNOWN" items generated by your AI engine earlier.

This completes the entire backend stack. You have a Data Ingest pipeline, a Diff Engine, an AI Classifier, and now a FastAPI Interface for human governance.

[Building a Production-Grade FastAPI Application](https://www.youtube.com/watch?v=kmJz8w5ij8Y)
This video is highly relevant as it covers essential best practices for structuring and deploying a FastAPI application like the BFF you are building, specifically addressing folder structure, database connections, and validation which directly complements the code provided.

