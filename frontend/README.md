This is a production-grade Frontend implementation using React (TypeScript) and Tailwind CSS. It is designed to be lightweight, fast, and strictly typed to match your Backend-for-Frontend (BFF) schemas.

## 1. Setup & Installation
Create a frontend/ folder in your project root using Vite (a modern, fast build tool).

```bash
npm create vite@latest frontend -- --template react-ts
cd frontend
npm install axios @tanstack/react-query lucide-react clsx tailwind-merge
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

### The Workstation Component (frontend/src/components/ReconWorkstation.tsx)
This is the core UI. It displays the "Before/After" comparison and provides the governance controls.

### Application Entry (frontend/src/App.tsx)
This wraps the application in the QueryClientProvider to manage API state caching and background refetching.


### Running the Full Stack
- **Backend:** Ensure that FastAPI server is running (python bff/main.py) or run via Docker Compose.
- **Frontend (dev):** `npm run dev`

#### Access (dev): Open http://localhost:5173.

You will see the list of "UNKNOWN" differences generated by your Python script. Clicking on one will show the Source A vs B values, the AI's low-confidence guess, and allow you to "Reject/Override," which will write back to the Postgres Audit Trail.

### Quick Commands (copy-paste)

- Start the full development stack with hot-reload (BFF + DB + Frontend dev server):

```bash
docker compose --profile dev up --build
```

- Start the production-like frontend (builds with multi-stage and serves with Nginx):

```bash
docker compose --profile prod up --build
```

- Run a single batch job (example: ingest):

```bash
docker compose --profile jobs run --rm ingest-job
```

# Productionsing
To "productionise" this system, we need to move from independent scripts and local servers to a robust, scalable, and observable architecture.

Below is the strategy to take this from Proof-of-Concept (POC) to Production.

## Architecture Transition
We need to decouple the monolithic scripts into containerized services managed by an orchestrator (Kubernetes or ECS).
| Component | Current State (POC) | Production State |
|---|---|---|
| Ingestion | `ingest_engine.py` (Script) | Ingest Service (Pod): Listens to S3 events or Kafka topics for new data. |
| Diff Engine | `diff_engine.py` (Script) | Diff Worker (Job): Triggered by Airflow/Prefect DAGs; scales horizontally based on record volume. |
| AI Engine | `attribution_engine.py` (Script) | Inference Service: A dedicated API (FastAPI) or batch job that loads the trained model from a Model Registry (MLflow). |
| BFF | `main.py` (Local Uvicorn) | BFF Deployment: Load balanced behind Nginx/Ingress with SSL termination and rate limiting. |
| Frontend | Vite Dev Server | Static Asset Hosting: Built React bundle hosted on S3/CloudFront (CDN). |

## 2. Infrastructure as Code (Terraform)
Do not manually create resources. Use Terraform to define the infrastructure.

Recommended Folder Structure:
```
infrastructure/
├── main.tf             # Core resources
├── database.tf         # RDS (Postgres) + Read Replicas
├── eks.tf              # Kubernetes Cluster
├── s3.tf               # Buckets for raw ingestion
└── iam.tf              # Roles and Policies
```

### Key Production Configurations:
- Database: Use AWS RDS (Postgres) or Azure Database for PostgreSQL. Enable Multi-AZ for high availability and Performance Insights.
- Secrets: Never store DB passwords in code. Use AWS Secrets Manager or HashiCorp Vault and inject them as environment variables at runtime.

## 3. Orchestration & Scheduling (Airflow)
A workflow orchestrator is required instead of manually running script `python script.py`.

### Apache Airflow or Prefect
#### Example DAG Logic
- **Sensor:** Wait for source_a.csv and source_b.csv to land in S3.
- **Task A:** Spin up Ingestion Container $\rightarrow$ Writes to DB.
- **Task B:** Spin up Diff Engine (Spark or Python Batch) $\rightarrow$ Writes Diffs.
- **Task C:** Spin up AI Attribute $\rightarrow$ Updates Diffs.
- **Task D:** Check "Quality Gate" $\rightarrow$ If pass, trigger downstream API.
## 4. CI/CD Pipelines (GitHub Actions / GitLab CI)
Automate testing, building, and deployment of each component.

### Pipeline Stages:
- **Lint & Test:** Run pytest on backend and npm test on frontend.
- **Security Scan:** Run trivy or snyk to check Docker images for vulnerabilities.
- **Build & Push:** Build Docker images and push to ECR/ACR with a unique tag (e.g., commit SHA).
- **Database Migration:** Run Alembic or SQL scripts to apply schema changes before deploying code.
- **Deploy:** Update the Kubernetes manifest (helm upgrade) with the new image tag.

## 5. Observability & Monitoring
In production to know when things break before users do.
- Logs: Aggregate logs using ELK Stack (Elasticsearch, Logstash, Kibana) or Datadog.
- Metrics: Expose Prometheus metrics from your FastAPI and Python jobs.
    - Key Metric: recon_records_processed_total
    - Key Metric: recon_unknown_rate (Alert if > 5%)
- Tracing: Use OpenTelemetry (Jaeger/Zipkin) to trace a request from the UI $\rightarrow$ BFF $\rightarrow$ DB.

## 6. Security Hardening
- **Identity Provider (IdP):** Integrate the BFF with Auth0, Okta, or Keycloak. Do not manage user passwords yourself.
- **RBAC:** Ensure only "Senior Operations" users can APPROVE or OVERRIDE in the UI.
- **Network Policies:** The Database should not be accessible from the public internet. Only the BFF and Worker Pods inside the VPC should have access.

## 7. Containerization
This configuration is designed for local development and testing with a path towards production. It utilizes Docker Compose Profiles to ensure you can run the batch jobs (Ingest/Diff/AI) on-demand without them cluttering your logs or restarting endlessly.

### i) Backend Dockerfile (Batch Jobs)
`backend/Dockerfile` packages the ingestion, diff, and AI engines.

### ii) BFF Dockerfile (API Service)
`bff/Dockerfile` runs the FastAPI server.

### iii) Frontend Dockerfile (Production Build)
`frontend/Dockerfile` uses a Multi-Stage Build. It builds the React app using Node.js, then copies the static files to a lightweight Nginx server.

### iv) The Master `$PROJECTROOT/docker-compose.yml`

#### Key Features:
- Profiles: The ingest, diff, and ai services have profiles: ["jobs"]. This means they will not start when you run docker compose up. You must trigger them manually.
- Dependencies: The BFF waits for the DB to be healthy before starting.
- Volume Mounting: Source code is mounted into the containers (./backend:/app/backend). This allows you to edit code locally and see changes instantly without rebuilding images.

### v) How to Use (The Independent Control You Requested)
This setup gives granular control over every component.

### A. Start the "Online" Stack
`docker compose up -d` brings up the Database, the BFF API, and the Frontend UI.

- UI: http://localhost:3000
- API Docs: http://localhost:8000/docs
- DB: localhost:5432

### B. Run the Batch Pipeline (One by One)
Since these are defined as `profiles: ["jobs"]`, they don't run automatically. You trigger them exactly when you want to test that specific logic.

i) Generate Data:
```bash
docker compose run --rm generate-data
```
ii) Ingest Data:
```bash
docker compose run --rm ingest-job
```

iii) Run Diff Engine:
```bash
docker compose run --rm diff-job
```

iv) Run AI Attribution:
```bash
docker compose run --rm ai-job
```

**Note:** The --rm flag automatically cleans up the container after the script finishes, keeping your environment clean.

### C. Resetting Everything (Complete Teardown)
If you want to wipe the database and start fresh (useful for testing seed scripts):

```bash
docker compose down -v
```

(The -v flag deletes the Postgres data volume, so the next up will re-run the 01_init... and 02_seed... scripts).

## Conclusion
This setup provides a solid foundation for developing, testing, and eventually productionising a data reconciliation system. By following best practices in containerization, orchestration, CI/CD, and observability, you can ensure that your system is robust, scalable, and maintainable.

## License
This project is licensed under the MIT License. See the LICENSE file for details.

## Additional
Based on the Python/Pandas and SQLAlchemy stack we have implemented, your system is architecturally capable of connecting to almost any modern data source.

Here is the breakdown of what is supported currently vs. what can be easily added:

1. File-Based & Object Storage (Data Lakes)
This is the most common pattern for high-volume reconciliation (e.g., reconciling 10M+ records).

Formats: CSV, Parquet (highly recommended for speed), Avro, JSON, Excel.

Locations:

Local Filesystem: (Current Implementation)

AWS S3 / Azure Blob / Google Cloud Storage: Supported natively by Pandas (pd.read_parquet('s3://bucket/data.parquet')) using libraries like s3fs.

SFTP: Common for legacy banking file transfers.

2. Relational Databases (Data Marts)
Since we are using SQLAlchemy, you can connect to any database that has a Python driver.

Systems: PostgreSQL, Oracle, Microsoft SQL Server, MySQL, MariaDB, DB2.

Method: You simply change the connection string in the ingestion engine.

Current: pd.read_csv(...)

Upgrade: pd.read_sql("SELECT * FROM trade_table", con=source_db_engine)

3. Cloud Data Warehouses
For enterprise scenarios where data sits in a warehouse.

Systems: Snowflake, Google BigQuery, Amazon Redshift, Databricks.

Method: These have specialized Python connectors (e.g., snowflake-connector-python) that integrate efficiently with Pandas.

4. APIs & Streams (Real-time/Near Real-time)
REST/GraphQL APIs: You can fetch data using the requests library and load it into a DataFrame.

Kafka / RabbitMQ: While reconciliation is typically batch-oriented, you can consume a stream of events, buffer them into a batch, and then run the reconciliation process.

How to extend your code?
In your backend/ingest_engine.py, you currently have:
```python
df_a = pd.read_csv(SOURCE_A_PATH)
```

To connect to a SQL Database (like an Oracle Data Mart), you would just modify that line:

```python
# Create connection to the Source Data Mart
source_engine = create_engine("oracle+cx_oracle://user:pass@host:1521/?service_name=orcl")

# Pull data directly using SQL
df_a = pd.read_sql("SELECT id, amount, currency FROM trades WHERE date = '2024-01-15'", source_engine)
```

